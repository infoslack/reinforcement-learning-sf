{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import retro\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env \n",
    "from gym.spaces import MultiBinary, Box\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52020f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'\n",
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "class StreetFighter(Env): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # action space and observation\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 100, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        # instance of the game \n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Create a attribute to hold the score delta \n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation): \n",
    "        # Grayscaling \n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize \n",
    "        resize = cv2.resize(gray, (100, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add the channels value\n",
    "        channels = np.reshape(resize, (100, 100, 1))\n",
    "        return channels \n",
    "    \n",
    "    def step(self, action): \n",
    "        # Take a step \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) \n",
    "        \n",
    "        # Frame delta off\n",
    "        frame_delta = obs\n",
    "        \n",
    "        # Reshape the reward function\n",
    "        reward = info['score'] - self.score \n",
    "        self.score = info['score'] \n",
    "        \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return test hyperparameters\n",
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 16384),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-7, 1e-3),\n",
    "        'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76349cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop\n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial)\n",
    "\n",
    "        # Environment \n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Create model\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "        # Evaluate model\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the experiment\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52009c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30306948",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex: load the best trial model\n",
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_9_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e555923",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ba917",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = 3648 # a factor of 64\n",
    "model_params['learning_rate'] = 4.694433938870559e-7\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "model.learn(total_timesteps=1000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d39f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = PPO.load('train/best_model_420000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ded93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "obs = env.reset()\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.005)\n",
    "        if reward > 0:\n",
    "            print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b027d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
